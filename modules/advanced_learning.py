#!/usr/bin/env python3
"""
ê³ ê¸‰ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ ëª¨ë“ˆ
ê°•í™”í•™ìŠµ, ì „ì´í•™ìŠµ, ë™ì  ì„ê³„ê°’ ì¡°ì • ë“± ê³ ê¸‰ í•™ìŠµ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import math
import random
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
from collections import defaultdict, deque

from .models import GuessResult, GameSession, SuccessPattern


class QLearningStrategy:
    """
    Q-Learning ê¸°ë°˜ ì „ëµ ì„ íƒ ìµœì í™”
    ê° ìƒí™©ì—ì„œ ìµœì ì˜ ì „ëµì„ í•™ìŠµí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, learning_rate: float = 0.1, discount_factor: float = 0.9, 
                 epsilon: float = 0.1):
        """
        Q-Learning íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        
        Args:
            learning_rate (float): í•™ìŠµë¥  (0.0 ~ 1.0)
            discount_factor (float): í• ì¸ ì¸ìˆ˜ (0.0 ~ 1.0)
            epsilon (float): íƒí—˜ë¥  (0.0 ~ 1.0)
        """
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        
        # Q-í…Œì´ë¸”: (ìƒíƒœ, ì•¡ì…˜) -> Qê°’
        self.q_table: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
        
        # ê°€ëŠ¥í•œ ì „ëµë“¤
        self.strategies = ["ë„“ì€ì˜ë¯¸íƒìƒ‰", "ì˜ë¯¸ì ê²½ì‚¬íƒìƒ‰", "ì§‘ì¤‘ì˜ë¯¸íƒìƒ‰", "ì •ë°€ì˜ë¯¸íƒìƒ‰"]
        
        # ìƒíƒœ íˆìŠ¤í† ë¦¬ (í•™ìŠµìš©)
        self.state_action_history: List[Tuple[str, str, float]] = []
    
    def get_state_key(self, session: GameSession) -> str:
        """
        í˜„ì¬ ê²Œì„ ìƒíƒœë¥¼ ë¬¸ìì—´ í‚¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            session (GameSession): í˜„ì¬ ê²Œì„ ì„¸ì…˜
            
        Returns:
            str: ìƒíƒœ í‚¤
        """
        if not session.guesses:
            return "ì´ˆê¸°ìƒíƒœ"
        
        best_similarity = session.get_best_similarity()
        attempts = len(session.guesses)
        is_stuck = session.is_stagnant()
        
        # ìƒíƒœë¥¼ ì¹´í…Œê³ ë¦¬í™”
        sim_category = "ë§¤ìš°ë‚®ìŒ" if best_similarity < 0.1 else \
                      "ë‚®ìŒ" if best_similarity < 0.25 else \
                      "ì¤‘ê°„" if best_similarity < 0.5 else \
                      "ë†’ìŒ"
        
        attempt_category = "ì´ˆê¸°" if attempts < 10 else \
                          "ì¤‘ê¸°" if attempts < 30 else \
                          "í›„ê¸°"
        
        stuck_status = "ì •ì²´" if is_stuck else "ì§„í–‰"
        
        return f"{sim_category}_{attempt_category}_{stuck_status}"
    
    def select_strategy(self, session: GameSession) -> str:
        """
        Q-Learningì„ ì‚¬ìš©í•˜ì—¬ ìµœì  ì „ëµì„ ì„ íƒí•©ë‹ˆë‹¤.
        
        Args:
            session (GameSession): í˜„ì¬ ê²Œì„ ì„¸ì…˜
            
        Returns:
            str: ì„ íƒëœ ì „ëµ ì´ë¦„
        """
        state = self.get_state_key(session)
        
        # Îµ-greedy ì •ì±…
        if random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ ì „ëµ ì„ íƒ
            strategy = random.choice(self.strategies)
        else:
            # ì´ìš©: ìµœê³  Qê°’ ì „ëµ ì„ íƒ
            q_values = self.q_table[state]
            if not q_values:
                # Qê°’ì´ ì—†ìœ¼ë©´ ëœë¤ ì„ íƒ
                strategy = random.choice(self.strategies)
            else:
                strategy = max(q_values.items(), key=lambda x: x[1])[0]
        
        # íˆìŠ¤í† ë¦¬ì— ê¸°ë¡
        self.state_action_history.append((state, strategy, 0.0))  # ë³´ìƒì€ ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
        
        return strategy
    
    def update_q_values(self, final_reward: float, session_length: int) -> None:
        """
        ê²Œì„ ì¢…ë£Œ í›„ Qê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            final_reward (float): ìµœì¢… ë³´ìƒ (ì„±ê³µ=1.0, ì‹¤íŒ¨=0.0)
            session_length (int): ê²Œì„ ì‹œë„ íšŸìˆ˜
        """
        # ë³´ìƒ ê³„ì‚° (ì‹œë„ íšŸìˆ˜ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ë³´ìƒ)
        efficiency_bonus = max(0, (100 - session_length) / 100)
        total_reward = final_reward + efficiency_bonus
        
        # ì—­ìˆœìœ¼ë¡œ Qê°’ ì—…ë°ì´íŠ¸ (ì‹œê°„ì°¨ í•™ìŠµ)
        for i in reversed(range(len(self.state_action_history))):
            state, action, _ = self.state_action_history[i]
            
            # í˜„ì¬ Qê°’
            current_q = self.q_table[state][action]
            
            # ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Qê°’ (ë§ˆì§€ë§‰ì´ë©´ 0)
            if i == len(self.state_action_history) - 1:
                max_next_q = 0
            else:
                next_state = self.state_action_history[i + 1][0]
                next_q_values = self.q_table[next_state]
                max_next_q = max(next_q_values.values()) if next_q_values else 0
            
            # Qê°’ ì—…ë°ì´íŠ¸
            new_q = current_q + self.learning_rate * (
                total_reward + self.discount_factor * max_next_q - current_q
            )
            self.q_table[state][action] = new_q
            
            # ë³´ìƒ ê°ì‡  (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì˜í–¥ ê°ì†Œ)
            total_reward *= 0.9
        
        # íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        self.state_action_history.clear()
    
    def get_q_statistics(self) -> Dict[str, Any]:
        """Q-Learning í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        stats = {
            "total_states": len(self.q_table),
            "total_state_actions": sum(len(actions) for actions in self.q_table.values()),
            "epsilon": self.epsilon,
            "learning_rate": self.learning_rate,
            "top_strategies_by_state": {}
        }
        
        for state, actions in self.q_table.items():
            if actions:
                best_strategy = max(actions.items(), key=lambda x: x[1])
                stats["top_strategies_by_state"][state] = {
                    "strategy": best_strategy[0],
                    "q_value": best_strategy[1]
                }
        
        return stats


class TransferLearning:
    """
    ì „ì´ í•™ìŠµ: ë‹¤ë¥¸ ê²Œì„ì´ë‚˜ ë„ë©”ì¸ì˜ ì§€ì‹ì„ í™œìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """ì „ì´ í•™ìŠµ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        # ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ (ì‚¬ì „ ì •ì˜ëœ ë‹¨ì–´ ê´€ê³„)
        self.external_knowledge = self._load_external_knowledge()
        
        # ë„ë©”ì¸ ê°„ ë§¤í•‘
        self.domain_mappings = self._create_domain_mappings()
        
        # ì „ì´ í•™ìŠµ ê°€ì¤‘ì¹˜
        self.transfer_weights = {
            "semantic_similarity": 0.3,  # ì˜ë¯¸ì  ìœ ì‚¬ì„±
            "morphological_similarity": 0.2,  # í˜•íƒœì  ìœ ì‚¬ì„±
            "contextual_similarity": 0.3,  # ë¬¸ë§¥ì  ìœ ì‚¬ì„±
            "frequency_pattern": 0.2  # ë¹ˆë„ íŒ¨í„´
        }
    
    def _load_external_knowledge(self) -> Dict[str, List[str]]:
        """
        ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì™¸ë¶€ APIë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        return {
            # ì‹œê°„ ê´€ë ¨ ë‹¨ì–´ë“¤
            "ì‹œê°„": ["ìˆœê°„", "ì‹œê¸°", "ë•Œ", "ì‹œì ˆ", "ê¸°ê°„", "ì‹œì ", "ì‹œëŒ€", "ë™ì•ˆ", "ì‚¬ì´"],
            
            # ê°ì • ê´€ë ¨ ë‹¨ì–´ë“¤
            "ê°ì •": ["ë§ˆìŒ", "ê¸°ë¶„", "ëŠë‚Œ", "ì •ì„œ", "ì‹¬ë¦¬", "ì˜ì‹", "ìƒê°", "ë§ˆìŒê°€ì§"],
            
            # ì‚¬íšŒ ê´€ë ¨ ë‹¨ì–´ë“¤
            "ì‚¬íšŒ": ["ì •ì¹˜", "ê²½ì œ", "ë¬¸í™”", "êµìœ¡", "ì œë„", "ê³µë™ì²´", "ì‹œë¯¼", "êµ­ë¯¼"],
            
            # ìì—° ê´€ë ¨ ë‹¨ì–´ë“¤
            "ìì—°": ["í™˜ê²½", "ìƒíƒœ", "ê¸°í›„", "ë‚ ì”¨", "ê³„ì ˆ", "ìƒë¬¼", "ì‹ë¬¼", "ë™ë¬¼"],
            
            # ê¸°ìˆ  ê´€ë ¨ ë‹¨ì–´ë“¤
            "ê¸°ìˆ ": ["ê³¼í•™", "ë°œëª…", "í˜ì‹ ", "ê°œë°œ", "ì—°êµ¬", "ì‹¤í—˜", "ë°œê²¬", "ì§„ë³´"]
        }
    
    def _create_domain_mappings(self) -> Dict[str, str]:
        """
        ì„œë¡œ ë‹¤ë¥¸ ë„ë©”ì¸ ê°„ì˜ ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        return {
            # ì¶”ìƒ -> êµ¬ì²´
            "ìƒê°": "ì•„ì´ë””ì–´",
            "ë§ˆìŒ": "ê°ì •",
            "ì‹œê°„": "ìˆœê°„",
            
            # ì¼ë°˜ -> ì „ë¬¸
            "ì—°êµ¬": "ì‹¤í—˜",
            "êµìœ¡": "í•™ìŠµ",
            "ë¬¸ì œ": "ê³¼ì œ",
            
            # ê³µì‹ -> ë¹„ê³µì‹
            "ì •ë¶€": "ë‚˜ë¼",
            "ì‹œë¯¼": "ì‚¬ëŒ",
            "ì œë„": "ê·œì¹™"
        }
    
    def get_transferred_candidates(self, current_word: str, 
                                 current_similarity: float) -> List[Tuple[str, float]]:
        """
        ì „ì´ í•™ìŠµì„ í†µí•´ í›„ë³´ ë‹¨ì–´ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            current_word (str): í˜„ì¬ ë‹¨ì–´
            current_similarity (float): í˜„ì¬ ìœ ì‚¬ë„
            
        Returns:
            List[Tuple[str, float]]: (ë‹¨ì–´, ì˜ˆìƒìœ ì‚¬ë„) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        """
        candidates = []
        
        # 1. ì˜ë¯¸ì  ì „ì´
        semantic_candidates = self._semantic_transfer(current_word, current_similarity)
        candidates.extend(semantic_candidates)
        
        # 2. í˜•íƒœì  ì „ì´
        morphological_candidates = self._morphological_transfer(current_word, current_similarity)
        candidates.extend(morphological_candidates)
        
        # 3. ë¬¸ë§¥ì  ì „ì´
        contextual_candidates = self._contextual_transfer(current_word, current_similarity)
        candidates.extend(contextual_candidates)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ í•©ì‚°
        word_scores = defaultdict(list)
        for word, score in candidates:
            word_scores[word].append(score)
        
        # í‰ê·  ì ìˆ˜ë¡œ ìµœì¢… í›„ë³´ ìƒì„±
        final_candidates = [
            (word, sum(scores) / len(scores))
            for word, scores in word_scores.items()
        ]
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        final_candidates.sort(key=lambda x: x[1], reverse=True)
        
        return final_candidates[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
    
    def _semantic_transfer(self, word: str, similarity: float) -> List[Tuple[str, float]]:
        """ì˜ë¯¸ì  ì „ì´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        candidates = []
        
        # ì™¸ë¶€ ì§€ì‹ì—ì„œ ê´€ë ¨ ë‹¨ì–´ ì°¾ê¸°
        for category, related_words in self.external_knowledge.items():
            if word in related_words or word == category:
                for related_word in related_words:
                    if related_word != word:
                        # ìœ ì‚¬ë„ ì¶”ì • (ì¹´í…Œê³ ë¦¬ ë‚´ ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ìœ ì‚¬ë„ë¥¼ ê°€ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)
                        estimated_similarity = similarity + random.uniform(-0.05, 0.05)
                        estimated_similarity = max(0, min(1, estimated_similarity))
                        candidates.append((related_word, estimated_similarity))
        
        return candidates
    
    def _morphological_transfer(self, word: str, similarity: float) -> List[Tuple[str, float]]:
        """í˜•íƒœì  ì „ì´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        candidates = []
        
        # ë„ë©”ì¸ ë§¤í•‘ í™œìš©
        if word in self.domain_mappings:
            mapped_word = self.domain_mappings[word]
            # ë§¤í•‘ëœ ë‹¨ì–´ëŠ” ì•½ê°„ ë‹¤ë¥¸ ìœ ì‚¬ë„ë¥¼ ê°€ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒ
            estimated_similarity = similarity + random.uniform(-0.1, 0.1)
            estimated_similarity = max(0, min(1, estimated_similarity))
            candidates.append((mapped_word, estimated_similarity))
        
        # ì—­ë°©í–¥ ë§¤í•‘ë„ í™•ì¸
        for source, target in self.domain_mappings.items():
            if word == target:
                estimated_similarity = similarity + random.uniform(-0.1, 0.1)
                estimated_similarity = max(0, min(1, estimated_similarity))
                candidates.append((source, estimated_similarity))
        
        return candidates
    
    def _contextual_transfer(self, word: str, similarity: float) -> List[Tuple[str, float]]:
        """ë¬¸ë§¥ì  ì „ì´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        candidates = []
        
        # ë‹¨ì–´ì˜ ê¸¸ì´ë‚˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì „ì´
        if len(word) >= 2:
            # ë¹„ìŠ·í•œ ê¸¸ì´ì˜ ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ìœ ì‚¬ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ
            for category_words in self.external_knowledge.values():
                for candidate_word in category_words:
                    if (abs(len(candidate_word) - len(word)) <= 1 and 
                        candidate_word != word):
                        # ê¸¸ì´ê°€ ë¹„ìŠ·í•œ ë‹¨ì–´ëŠ” ì•½ê°„ì˜ ìœ ì‚¬ë„ ë³€í™”
                        estimated_similarity = similarity + random.uniform(-0.03, 0.03)
                        estimated_similarity = max(0, min(1, estimated_similarity))
                        candidates.append((candidate_word, estimated_similarity))
        
        return candidates


class AdaptiveThresholds:
    """
    ì ì‘ì  ì„ê³„ê°’: ì„±ëŠ¥ì— ë”°ë¼ ì „ëµ ì „í™˜ ì„ê³„ê°’ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """ì ì‘ì  ì„ê³„ê°’ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ ì„ê³„ê°’
        self.base_thresholds = {
            "wide_to_gradient": 0.1,
            "gradient_to_focused": 0.25,
            "focused_to_precision": 0.5
        }
        
        # í˜„ì¬ ì„ê³„ê°’ (ë™ì ìœ¼ë¡œ ì¡°ì •ë¨)
        self.current_thresholds = self.base_thresholds.copy()
        
        # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬
        self.performance_history = deque(maxlen=50)  # ìµœê·¼ 50ê²Œì„ ê¸°ë¡
        
        # ì¡°ì • ë§¤ê°œë³€ìˆ˜
        self.adjustment_rate = 0.02  # ì„ê³„ê°’ ì¡°ì • í­
        self.success_rate_target = 0.7  # ëª©í‘œ ì„±ê³µë¥ 
    
    def update_performance(self, success: bool, attempts: int, 
                         strategy_transitions: List[Tuple[str, float]]) -> None:
        """
        ê²Œì„ ì„±ê³¼ë¥¼ ê¸°ë¡í•˜ê³  ì„ê³„ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤.
        
        Args:
            success (bool): ê²Œì„ ì„±ê³µ ì—¬ë¶€
            attempts (int): ì‹œë„ íšŸìˆ˜
            strategy_transitions (List[Tuple[str, float]]): (ì „ëµëª…, ì „í™˜ì‹œì ìœ ì‚¬ë„) ë¦¬ìŠ¤íŠ¸
        """
        # ì„±ê³¼ ê¸°ë¡
        performance_record = {
            "success": success,
            "attempts": attempts,
            "efficiency": 1.0 / attempts if attempts > 0 else 0,
            "strategy_transitions": strategy_transitions,
            "timestamp": datetime.now()
        }
        
        self.performance_history.append(performance_record)
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìŒ“ì´ë©´ ì„ê³„ê°’ ì¡°ì •
        if len(self.performance_history) >= 10:
            self._adjust_thresholds()
    
    def _adjust_thresholds(self) -> None:
        """ì„±ëŠ¥ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ê³„ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤."""
        recent_games = list(self.performance_history)[-20:]  # ìµœê·¼ 20ê²Œì„
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = sum(game["success"] for game in recent_games) / len(recent_games)
        
        # í‰ê·  ì‹œë„ íšŸìˆ˜ ê³„ì‚°
        avg_attempts = sum(game["attempts"] for game in recent_games) / len(recent_games)
        
        # ì „ëµë³„ ì„±ê³¼ ë¶„ì„
        strategy_performance = self._analyze_strategy_performance(recent_games)
        
        # ì„ê³„ê°’ ì¡°ì • ë¡œì§
        if success_rate < self.success_rate_target:
            # ì„±ê³µë¥ ì´ ë‚®ìœ¼ë©´ ë” ë¹¨ë¦¬ ê³ ê¸‰ ì „ëµìœ¼ë¡œ ì „í™˜
            self._decrease_thresholds()
        elif avg_attempts < 20:
            # ì‹œë„ íšŸìˆ˜ê°€ ì ìœ¼ë©´ í˜„ì¬ ì„ê³„ê°’ì´ ì¢‹ìŒ (ìœ ì§€)
            pass
        else:
            # ì‹œë„ íšŸìˆ˜ê°€ ë§ìœ¼ë©´ ì „ëµ ì „í™˜ì„ ëŠ¦ì¶¤
            self._increase_thresholds()
        
        print(f"ğŸ“Š ì„ê³„ê°’ ì¡°ì •: {self.current_thresholds}")
    
    def _analyze_strategy_performance(self, games: List[Dict]) -> Dict[str, float]:
        """ì „ëµë³„ ì„±ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        strategy_stats = defaultdict(list)
        
        for game in games:
            for strategy, similarity in game["strategy_transitions"]:
                strategy_stats[strategy].append({
                    "success": game["success"],
                    "efficiency": game["efficiency"],
                    "similarity_at_transition": similarity
                })
        
        # ì „ëµë³„ í‰ê·  ì„±ê³¼ ê³„ì‚°
        strategy_performance = {}
        for strategy, records in strategy_stats.items():
            if records:
                avg_success = sum(r["success"] for r in records) / len(records)
                avg_efficiency = sum(r["efficiency"] for r in records) / len(records)
                strategy_performance[strategy] = avg_success * avg_efficiency
        
        return strategy_performance
    
    def _decrease_thresholds(self) -> None:
        """ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” ë¹¨ë¦¬ ê³ ê¸‰ ì „ëµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤."""
        for key in self.current_thresholds:
            self.current_thresholds[key] = max(
                0.05,  # ìµœì†Œê°’
                self.current_thresholds[key] - self.adjustment_rate
            )
    
    def _increase_thresholds(self) -> None:
        """ì„ê³„ê°’ì„ ë†’ì—¬ì„œ ì „ëµ ì „í™˜ì„ ëŠ¦ì¶¥ë‹ˆë‹¤."""
        for key in self.current_thresholds:
            self.current_thresholds[key] = min(
                0.8,  # ìµœëŒ€ê°’
                self.current_thresholds[key] + self.adjustment_rate
            )
    
    def get_threshold(self, transition_type: str) -> float:
        """
        ì§€ì •ëœ ì „í™˜ íƒ€ì…ì˜ í˜„ì¬ ì„ê³„ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            transition_type (str): ì „í™˜ íƒ€ì… í‚¤
            
        Returns:
            float: í˜„ì¬ ì„ê³„ê°’
        """
        return self.current_thresholds.get(transition_type, 0.25)
    
    def get_adaptation_statistics(self) -> Dict[str, Any]:
        """ì ì‘ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.performance_history:
            return {"message": "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        recent_games = list(self.performance_history)[-10:]
        
        stats = {
            "current_thresholds": self.current_thresholds.copy(),
            "base_thresholds": self.base_thresholds.copy(),
            "recent_success_rate": sum(g["success"] for g in recent_games) / len(recent_games),
            "recent_avg_attempts": sum(g["attempts"] for g in recent_games) / len(recent_games),
            "total_games_recorded": len(self.performance_history),
            "threshold_adjustments": {
                key: round(current - self.base_thresholds[key], 4)
                for key, current in self.current_thresholds.items()
            }
        }
        
        return stats


class AdvancedLearningEngine:
    """
    ê³ ê¸‰ í•™ìŠµ ì—”ì§„: ëª¨ë“  ê³ ê¸‰ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, base_learning_engine):
        """
        ê³ ê¸‰ í•™ìŠµ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            base_learning_engine: ê¸°ë³¸ í•™ìŠµ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
        """
        self.base_engine = base_learning_engine
        
        # ê³ ê¸‰ í•™ìŠµ ì»´í¬ë„ŒíŠ¸ë“¤
        self.q_learning = QLearningStrategy()
        self.transfer_learning = TransferLearning()
        self.adaptive_thresholds = AdaptiveThresholds()
        
        # ì„¤ì •
        self.enable_q_learning = True
        self.enable_transfer_learning = True
        self.enable_adaptive_thresholds = True
    
    def select_enhanced_strategy(self, session: GameSession) -> str:
        """
        ê³ ê¸‰ í•™ìŠµì„ í™œìš©í•˜ì—¬ ì „ëµì„ ì„ íƒí•©ë‹ˆë‹¤.
        
        Args:
            session (GameSession): í˜„ì¬ ê²Œì„ ì„¸ì…˜
            
        Returns:
            str: ì„ íƒëœ ì „ëµ ì´ë¦„
        """
        if self.enable_q_learning and len(self.q_learning.state_action_history) > 5:
            # Q-Learningìœ¼ë¡œ ì „ëµ ì„ íƒ
            return self.q_learning.select_strategy(session)
        else:
            # ê¸°ë³¸ ì „ëµ ì„ íƒ ë¡œì§ ì‚¬ìš©
            best_similarity = session.get_best_similarity()
            
            # ì ì‘ì  ì„ê³„ê°’ ì‚¬ìš©
            if self.enable_adaptive_thresholds:
                wide_threshold = self.adaptive_thresholds.get_threshold("wide_to_gradient")
                gradient_threshold = self.adaptive_thresholds.get_threshold("gradient_to_focused")
                focused_threshold = self.adaptive_thresholds.get_threshold("focused_to_precision")
            else:
                wide_threshold, gradient_threshold, focused_threshold = 0.1, 0.25, 0.5
            
            is_stuck = session.is_stagnant()
            
            if best_similarity < wide_threshold:
                return "ë„“ì€ì˜ë¯¸íƒìƒ‰"
            elif best_similarity < gradient_threshold or is_stuck:
                return "ì˜ë¯¸ì ê²½ì‚¬íƒìƒ‰"
            elif best_similarity < focused_threshold:
                return "ì§‘ì¤‘ì˜ë¯¸íƒìƒ‰"
            else:
                return "ì •ë°€ì˜ë¯¸íƒìƒ‰"
    
    def get_enhanced_word_candidates(self, current_word: str, 
                                   current_similarity: float) -> List[Tuple[str, float]]:
        """
        ì „ì´ í•™ìŠµì„ í™œìš©í•˜ì—¬ í–¥ìƒëœ ë‹¨ì–´ í›„ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            current_word (str): í˜„ì¬ ë‹¨ì–´
            current_similarity (float): í˜„ì¬ ìœ ì‚¬ë„
            
        Returns:
            List[Tuple[str, float]]: (ë‹¨ì–´, ì˜ˆìƒìœ ì‚¬ë„) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        """
        if self.enable_transfer_learning:
            return self.transfer_learning.get_transferred_candidates(
                current_word, current_similarity)
        else:
            return []
    
    def update_advanced_learning(self, session: GameSession, success: bool, 
                               final_answer: str = None) -> None:
        """
        ê²Œì„ ì¢…ë£Œ í›„ ëª¨ë“  ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            session (GameSession): ì™„ë£Œëœ ê²Œì„ ì„¸ì…˜
            success (bool): ì„±ê³µ ì—¬ë¶€
            final_answer (str): ìµœì¢… ì •ë‹µ (ì„±ê³µí•œ ê²½ìš°)
        """
        # Q-Learning ì—…ë°ì´íŠ¸
        if self.enable_q_learning:
            final_reward = 1.0 if success else 0.0
            self.q_learning.update_q_values(final_reward, len(session.guesses))
        
        # ì ì‘ì  ì„ê³„ê°’ ì—…ë°ì´íŠ¸
        if self.enable_adaptive_thresholds:
            # ì „ëµ ì „í™˜ ê¸°ë¡ ìƒì„±
            strategy_transitions = []
            for i, strategy in enumerate(session.strategy_history):
                # ê° ì „ëµì´ ì‚¬ìš©ëœ ì‹œì ì˜ ìœ ì‚¬ë„ ì¶”ì •
                if i < len(session.guesses):
                    similarity_at_transition = session.guesses[i].similarity
                else:
                    similarity_at_transition = session.get_best_similarity()
                strategy_transitions.append((strategy, similarity_at_transition))
            
            self.adaptive_thresholds.update_performance(
                success, len(session.guesses), strategy_transitions)
    
    def get_comprehensive_statistics(self) -> Dict[str, Any]:
        """ëª¨ë“  ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œì˜ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        stats = {
            "base_learning": self.base_engine.get_learning_statistics(),
            "advanced_features": {
                "q_learning_enabled": self.enable_q_learning,
                "transfer_learning_enabled": self.enable_transfer_learning,
                "adaptive_thresholds_enabled": self.enable_adaptive_thresholds
            }
        }
        
        if self.enable_q_learning:
            stats["q_learning"] = self.q_learning.get_q_statistics()
        
        if self.enable_adaptive_thresholds:
            stats["adaptive_thresholds"] = self.adaptive_thresholds.get_adaptation_statistics()
        
        if self.enable_transfer_learning:
            stats["transfer_learning"] = {
                "external_knowledge_categories": len(self.transfer_learning.external_knowledge),
                "domain_mappings": len(self.transfer_learning.domain_mappings)
            }
        
        return stats