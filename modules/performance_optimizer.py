#!/usr/bin/env python3
"""
ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆ
ë³‘ë ¬ ì²˜ë¦¬, ìºì‹±, ë°°ì¹˜ í•™ìŠµ ë“±ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import threading
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Callable, Any, Optional, Tuple
from functools import lru_cache, wraps
import pickle
import hashlib
from collections import defaultdict
from dataclasses import dataclass

from .models import GuessResult, GameSession


@dataclass
class CacheConfig:
    """ìºì‹œ ì„¤ì •ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    max_size: int = 1000
    ttl_seconds: int = 3600  # 1ì‹œê°„
    enable_disk_cache: bool = True
    cache_directory: str = ".cache"


class AdvancedCache:
    """
    ê³ ê¸‰ ìºì‹± ì‹œìŠ¤í…œ
    ë©”ëª¨ë¦¬ì™€ ë””ìŠ¤í¬ ìºì‹±ì„ ì§€ì›í•˜ë©° TTL(Time To Live)ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: CacheConfig = None):
        """
        ìºì‹œ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            config (CacheConfig): ìºì‹œ ì„¤ì •
        """
        self.config = config or CacheConfig()
        self.memory_cache: Dict[str, Dict] = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "disk_hits": 0,
            "disk_misses": 0
        }
        
        # ë””ìŠ¤í¬ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.config.enable_disk_cache:
            import os
            os.makedirs(self.config.cache_directory, exist_ok=True)
    
    def _generate_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """ìºì‹œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        key_data = f"{func_name}:{args}:{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _is_expired(self, cache_entry: Dict) -> bool:
        """ìºì‹œ í•­ëª©ì´ ë§Œë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return time.time() - cache_entry["timestamp"] > self.config.ttl_seconds
    
    def get(self, key: str) -> Optional[Any]:
        """
        ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            key (str): ìºì‹œ í‚¤
            
        Returns:
            Optional[Any]: ìºì‹œëœ ê°’ ë˜ëŠ” None
        """
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if key in self.memory_cache:
            entry = self.memory_cache[key]
            if not self._is_expired(entry):
                self.cache_stats["hits"] += 1
                return entry["value"]
            else:
                del self.memory_cache[key]
        
        # ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
        if self.config.enable_disk_cache:
            disk_path = f"{self.config.cache_directory}/{key}.pkl"
            try:
                import os
                if os.path.exists(disk_path):
                    with open(disk_path, 'rb') as f:
                        entry = pickle.load(f)
                    
                    if not self._is_expired(entry):
                        # ë©”ëª¨ë¦¬ ìºì‹œë¡œ ë³µì›
                        self.memory_cache[key] = entry
                        self.cache_stats["disk_hits"] += 1
                        return entry["value"]
                    else:
                        os.remove(disk_path)
            except Exception:
                pass
        
        self.cache_stats["misses"] += 1
        return None
    
    def set(self, key: str, value: Any) -> None:
        """
        ìºì‹œì— ê°’ì„ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            key (str): ìºì‹œ í‚¤
            value (Any): ì €ì¥í•  ê°’
        """
        entry = {
            "value": value,
            "timestamp": time.time()
        }
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
        self.memory_cache[key] = entry
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.memory_cache) > self.config.max_size:
            # ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU ë°©ì‹)
            oldest_key = min(self.memory_cache.keys(), 
                           key=lambda k: self.memory_cache[k]["timestamp"])
            del self.memory_cache[oldest_key]
        
        # ë””ìŠ¤í¬ ìºì‹œ ì €ì¥
        if self.config.enable_disk_cache:
            disk_path = f"{self.config.cache_directory}/{key}.pkl"
            try:
                with open(disk_path, 'wb') as f:
                    pickle.dump(entry, f)
            except Exception:
                pass
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = self.cache_stats["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            "hit_rate": round(hit_rate * 100, 2),
            "memory_cache_size": len(self.memory_cache),
            "stats": self.cache_stats.copy()
        }


def cached(cache_instance: AdvancedCache, ttl: int = None):
    """
    í•¨ìˆ˜ ìºì‹± ë°ì½”ë ˆì´í„°
    
    Args:
        cache_instance (AdvancedCache): ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
        ttl (int): TTL ì˜¤ë²„ë¼ì´ë“œ (ì´ˆ)
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = cache_instance._generate_cache_key(
                func.__name__, args, kwargs)
            
            # ìºì‹œì—ì„œ í™•ì¸
            cached_result = cache_instance.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # í•¨ìˆ˜ ì‹¤í–‰ ë° ìºì‹œ ì €ì¥
            result = func(*args, **kwargs)
            cache_instance.set(cache_key, result)
            
            return result
        return wrapper
    return decorator


class ParallelProcessor:
    """
    ë³‘ë ¬ ì²˜ë¦¬ ì—”ì§„
    ë‹¨ì–´ í›„ë³´ í‰ê°€, ì „ëµ ì‹¤í–‰ ë“±ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, max_workers: int = None):
        """
        ë³‘ë ¬ ì²˜ë¦¬ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            max_workers (int): ìµœëŒ€ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜)
        """
        import os
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.thread_pool = ThreadPoolExecutor(max_workers=self.max_workers)
        self.process_pool = ProcessPoolExecutor(max_workers=min(4, self.max_workers))
    
    def evaluate_candidates_parallel(self, candidates: List[str], 
                                   evaluation_func: Callable[[str], float],
                                   use_processes: bool = False) -> List[Tuple[str, float]]:
        """
        í›„ë³´ ë‹¨ì–´ë“¤ì„ ë³‘ë ¬ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            candidates (List[str]): í‰ê°€í•  í›„ë³´ ë‹¨ì–´ë“¤
            evaluation_func (Callable): í‰ê°€ í•¨ìˆ˜
            use_processes (bool): í”„ë¡œì„¸ìŠ¤ í’€ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            List[Tuple[str, float]]: (ë‹¨ì–´, ì ìˆ˜) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates:
            return []
        
        executor = self.process_pool if use_processes else self.thread_pool
        
        # ë³‘ë ¬ ì‘ì—… ì œì¶œ
        future_to_word = {
            executor.submit(evaluation_func, word): word 
            for word in candidates
        }
        
        results = []
        for future in as_completed(future_to_word):
            word = future_to_word[future]
            try:
                score = future.result(timeout=5)  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
                results.append((word, score))
            except Exception as e:
                print(f"âš ï¸ ë‹¨ì–´ '{word}' í‰ê°€ ì‹¤íŒ¨: {e}")
                results.append((word, 0.0))  # ì‹¤íŒ¨ì‹œ 0ì 
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x[1], reverse=True)
        return results
    
    def batch_strategy_evaluation(self, session: GameSession, 
                                strategies: List[str],
                                strategy_funcs: Dict[str, Callable]) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ì „ëµì„ ë³‘ë ¬ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            session (GameSession): í˜„ì¬ ì„¸ì…˜
            strategies (List[str]): í‰ê°€í•  ì „ëµë“¤
            strategy_funcs (Dict[str, Callable]): ì „ëµë³„ í•¨ìˆ˜ë“¤
            
        Returns:
            Dict[str, Any]: ì „ëµë³„ í‰ê°€ ê²°ê³¼
        """
        future_to_strategy = {
            self.thread_pool.submit(strategy_funcs[strategy], session): strategy
            for strategy in strategies if strategy in strategy_funcs
        }
        
        results = {}
        for future in as_completed(future_to_strategy):
            strategy = future_to_strategy[future]
            try:
                result = future.result(timeout=10)
                results[strategy] = result
            except Exception as e:
                print(f"âš ï¸ ì „ëµ '{strategy}' í‰ê°€ ì‹¤íŒ¨: {e}")
                results[strategy] = None
        
        return results
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)


class BatchLearning:
    """
    ë°°ì¹˜ í•™ìŠµ ì‹œìŠ¤í…œ
    íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ í†µí•´ í•™ìŠµ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """
    
    def __init__(self, batch_size: int = 10, update_interval: float = 5.0):
        """
        ë°°ì¹˜ í•™ìŠµ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            update_interval (float): ì—…ë°ì´íŠ¸ ê°„ê²© (ì´ˆ)
        """
        self.batch_size = batch_size
        self.update_interval = update_interval
        
        # ë°°ì¹˜ ë²„í¼
        self.word_relationship_batch: List[Dict] = []
        self.frequency_update_batch: List[Dict] = []
        self.pattern_update_batch: List[Dict] = []
        
        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
        self.last_update_time = time.time()
        
        # í†µê³„
        self.batch_stats = {
            "total_batches_processed": 0,
            "avg_batch_processing_time": 0.0,
            "pending_updates": 0
        }
    
    def add_word_relationship_update(self, word1: str, word2: str, 
                                   similarity_diff: float) -> None:
        """
        ë‹¨ì–´ ê´€ê³„ ì—…ë°ì´íŠ¸ë¥¼ ë°°ì¹˜ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            word1 (str): ì²« ë²ˆì§¸ ë‹¨ì–´
            word2 (str): ë‘ ë²ˆì§¸ ë‹¨ì–´
            similarity_diff (float): ìœ ì‚¬ë„ ì°¨ì´
        """
        update = {
            "type": "word_relationship",
            "word1": word1,
            "word2": word2,
            "similarity_diff": similarity_diff,
            "timestamp": time.time()
        }
        
        self.word_relationship_batch.append(update)
        self._check_batch_size()
    
    def add_frequency_update(self, word: str, similarity: float) -> None:
        """
        ë‹¨ì–´ ë¹ˆë„ ì—…ë°ì´íŠ¸ë¥¼ ë°°ì¹˜ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            word (str): ë‹¨ì–´
            similarity (float): ìœ ì‚¬ë„
        """
        update = {
            "type": "frequency",
            "word": word,
            "similarity": similarity,
            "timestamp": time.time()
        }
        
        self.frequency_update_batch.append(update)
        self._check_batch_size()
    
    def add_pattern_update(self, pattern_data: Dict) -> None:
        """
        íŒ¨í„´ ì—…ë°ì´íŠ¸ë¥¼ ë°°ì¹˜ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            pattern_data (Dict): íŒ¨í„´ ë°ì´í„°
        """
        update = {
            "type": "pattern",
            "data": pattern_data,
            "timestamp": time.time()
        }
        
        self.pattern_update_batch.append(update)
        self._check_batch_size()
    
    def _check_batch_size(self) -> None:
        """ë°°ì¹˜ í¬ê¸°ë¥¼ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        total_pending = (len(self.word_relationship_batch) + 
                        len(self.frequency_update_batch) + 
                        len(self.pattern_update_batch))
        
        current_time = time.time()
        time_elapsed = current_time - self.last_update_time
        
        # ë°°ì¹˜ í¬ê¸°ë‚˜ ì‹œê°„ ì¡°ê±´ ë§Œì¡±ì‹œ ì²˜ë¦¬
        if total_pending >= self.batch_size or time_elapsed >= self.update_interval:
            self._process_batches()
    
    def _process_batches(self) -> None:
        """ëª¨ë“  ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        start_time = time.time()
        
        # ë‹¨ì–´ ê´€ê³„ ë°°ì¹˜ ì²˜ë¦¬
        if self.word_relationship_batch:
            self._process_word_relationship_batch()
        
        # ë¹ˆë„ ì—…ë°ì´íŠ¸ ë°°ì¹˜ ì²˜ë¦¬
        if self.frequency_update_batch:
            self._process_frequency_batch()
        
        # íŒ¨í„´ ì—…ë°ì´íŠ¸ ë°°ì¹˜ ì²˜ë¦¬
        if self.pattern_update_batch:
            self._process_pattern_batch()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        processing_time = time.time() - start_time
        self.batch_stats["total_batches_processed"] += 1
        self.batch_stats["avg_batch_processing_time"] = (
            (self.batch_stats["avg_batch_processing_time"] * 
             (self.batch_stats["total_batches_processed"] - 1) + processing_time) /
            self.batch_stats["total_batches_processed"]
        )
        
        self.last_update_time = time.time()
        
        print(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.3f}ì´ˆ")
    
    def _process_word_relationship_batch(self) -> None:
        """ë‹¨ì–´ ê´€ê³„ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # ê°™ì€ ë‹¨ì–´ ìŒì˜ ì—…ë°ì´íŠ¸ë“¤ì„ ê·¸ë£¹í™”
        pair_updates = defaultdict(list)
        
        for update in self.word_relationship_batch:
            pair_key = f"{min(update['word1'], update['word2'])}|{max(update['word1'], update['word2'])}"
            pair_updates[pair_key].append(update['similarity_diff'])
        
        # ê·¸ë£¹í™”ëœ ì—…ë°ì´íŠ¸ ì ìš©
        for pair_key, similarity_diffs in pair_updates.items():
            # ì‹¤ì œ í•™ìŠµ ì—”ì§„ì— ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì ìš©
            # (ì—¬ê¸°ì„œëŠ” ì¸í„°í˜ì´ìŠ¤ë§Œ ì •ì˜)
            pass
        
        self.word_relationship_batch.clear()
    
    def _process_frequency_batch(self) -> None:
        """ë¹ˆë„ ì—…ë°ì´íŠ¸ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # ê°™ì€ ë‹¨ì–´ì˜ ì—…ë°ì´íŠ¸ë“¤ì„ ê·¸ë£¹í™”
        word_updates = defaultdict(list)
        
        for update in self.frequency_update_batch:
            word_updates[update['word']].append(update['similarity'])
        
        # ê·¸ë£¹í™”ëœ ì—…ë°ì´íŠ¸ ì ìš©
        for word, similarities in word_updates.items():
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° ë“±ì˜ íš¨ìœ¨ì ì¸ ì—…ë°ì´íŠ¸
            pass
        
        self.frequency_update_batch.clear()
    
    def _process_pattern_batch(self) -> None:
        """íŒ¨í„´ ì—…ë°ì´íŠ¸ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # íŒ¨í„´ ë°ì´í„° ì¼ê´„ ì²˜ë¦¬
        for update in self.pattern_update_batch:
            # íŒ¨í„´ ë¶„ì„ ë° ì €ì¥
            pass
        
        self.pattern_update_batch.clear()
    
    def force_process_all(self) -> None:
        """ëª¨ë“  ëŒ€ê¸° ì¤‘ì¸ ë°°ì¹˜ë¥¼ ê°•ì œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if (self.word_relationship_batch or 
            self.frequency_update_batch or 
            self.pattern_update_batch):
            self._process_batches()
    
    def get_batch_statistics(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        pending_updates = (len(self.word_relationship_batch) + 
                          len(self.frequency_update_batch) + 
                          len(self.pattern_update_batch))
        
        stats = self.batch_stats.copy()
        stats["pending_updates"] = pending_updates
        stats["time_since_last_update"] = time.time() - self.last_update_time
        
        return stats


class PerformanceMonitor:
    """
    ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
    ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ìµœì í™” ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.performance_data = {
            "function_times": defaultdict(list),
            "memory_usage": [],
            "cache_performance": {},
            "parallel_processing": {},
            "batch_processing": {}
        }
        
        self.monitoring_active = True
        self.start_time = time.time()
    
    def profile_function(self, func_name: str = None):
        """
        í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ì„ í”„ë¡œíŒŒì¼ë§í•˜ëŠ” ë°ì½”ë ˆì´í„°
        
        Args:
            func_name (str): í•¨ìˆ˜ ì´ë¦„ (Noneì´ë©´ ì‹¤ì œ í•¨ìˆ˜ëª… ì‚¬ìš©)
        """
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                if not self.monitoring_active:
                    return func(*args, **kwargs)
                
                name = func_name or func.__name__
                start_time = time.time()
                
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    execution_time = time.time() - start_time
                    self.performance_data["function_times"][name].append(execution_time)
                    
                    # ìµœê·¼ 100ê°œ ê¸°ë¡ë§Œ ìœ ì§€
                    if len(self.performance_data["function_times"][name]) > 100:
                        self.performance_data["function_times"][name] = \
                            self.performance_data["function_times"][name][-50:]
            
            return wrapper
        return decorator
    
    def record_memory_usage(self) -> None:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            
            memory_data = {
                "timestamp": time.time(),
                "rss_mb": memory_info.rss / 1024 / 1024,  # MB ë‹¨ìœ„
                "vms_mb": memory_info.vms / 1024 / 1024   # MB ë‹¨ìœ„
            }
            
            self.performance_data["memory_usage"].append(memory_data)
            
            # ìµœê·¼ 1000ê°œ ê¸°ë¡ë§Œ ìœ ì§€
            if len(self.performance_data["memory_usage"]) > 1000:
                self.performance_data["memory_usage"] = \
                    self.performance_data["memory_usage"][-500:]
                    
        except ImportError:
            # psutilì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë³´ë§Œ ê¸°ë¡
            self.performance_data["memory_usage"].append({
                "timestamp": time.time(),
                "rss_mb": "N/A",
                "vms_mb": "N/A"
            })
    
    def update_cache_performance(self, cache_stats: Dict) -> None:
        """ìºì‹œ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.performance_data["cache_performance"] = cache_stats
    
    def update_parallel_performance(self, parallel_stats: Dict) -> None:
        """ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.performance_data["parallel_processing"] = parallel_stats
    
    def update_batch_performance(self, batch_stats: Dict) -> None:
        """ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.performance_data["batch_processing"] = batch_stats
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ìƒì„¸í•œ ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        current_time = time.time()
        uptime = current_time - self.start_time
        
        # í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ í†µê³„
        function_stats = {}
        for func_name, times in self.performance_data["function_times"].items():
            if times:
                function_stats[func_name] = {
                    "avg_time": sum(times) / len(times),
                    "max_time": max(times),
                    "min_time": min(times),
                    "call_count": len(times),
                    "total_time": sum(times)
                }
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„
        memory_stats = {}
        if self.performance_data["memory_usage"]:
            recent_memory = self.performance_data["memory_usage"][-10:]
            if recent_memory and recent_memory[0]["rss_mb"] != "N/A":
                memory_stats = {
                    "current_rss_mb": recent_memory[-1]["rss_mb"],
                    "avg_rss_mb": sum(m["rss_mb"] for m in recent_memory) / len(recent_memory),
                    "peak_rss_mb": max(m["rss_mb"] for m in self.performance_data["memory_usage"])
                }
        
        report = {
            "uptime_seconds": uptime,
            "function_performance": function_stats,
            "memory_performance": memory_stats,
            "cache_performance": self.performance_data["cache_performance"],
            "parallel_performance": self.performance_data["parallel_processing"],
            "batch_performance": self.performance_data["batch_processing"],
            "optimization_suggestions": self._generate_optimization_suggestions()
        }
        
        return report
    
    def _generate_optimization_suggestions(self) -> List[str]:
        """ì„±ëŠ¥ ìµœì í™” ì œì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤."""
        suggestions = []
        
        # í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        function_times = self.performance_data["function_times"]
        for func_name, times in function_times.items():
            if times:
                avg_time = sum(times) / len(times)
                if avg_time > 1.0:  # 1ì´ˆ ì´ìƒ
                    suggestions.append(f"'{func_name}' í•¨ìˆ˜ ìµœì í™” ê³ ë ¤ (í‰ê·  {avg_time:.2f}ì´ˆ)")
        
        # ìºì‹œ ì„±ëŠ¥ ë¶„ì„
        cache_perf = self.performance_data["cache_performance"]
        if "hit_rate" in cache_perf and cache_perf["hit_rate"] < 50:
            suggestions.append(f"ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ í•„ìš” (í˜„ì¬ {cache_perf['hit_rate']}%)")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        memory_usage = self.performance_data["memory_usage"]
        if memory_usage and memory_usage[-1]["rss_mb"] != "N/A":
            current_memory = memory_usage[-1]["rss_mb"]
            if current_memory > 500:  # 500MB ì´ìƒ
                suggestions.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ê³ ë ¤ (í˜„ì¬ {current_memory:.1f}MB)")
        
        return suggestions
    
    def start_monitoring(self) -> None:
        """ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        self.monitoring_active = True
    
    def stop_monitoring(self) -> None:
        """ëª¨ë‹ˆí„°ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.monitoring_active = False


class PerformanceOptimizer:
    """
    ì„±ëŠ¥ ìµœì í™” í†µí•© ì‹œìŠ¤í…œ
    ëª¨ë“  ì„±ëŠ¥ ê°œì„  ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, cache_config: CacheConfig = None, 
                 max_workers: int = None, batch_size: int = 10):
        """
        ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            cache_config (CacheConfig): ìºì‹œ ì„¤ì •
            max_workers (int): ë³‘ë ¬ ì²˜ë¦¬ ìµœëŒ€ ì›Œì»¤ ìˆ˜
            batch_size (int): ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        """
        self.cache = AdvancedCache(cache_config)
        self.parallel_processor = ParallelProcessor(max_workers)
        self.batch_learning = BatchLearning(batch_size)
        self.monitor = PerformanceMonitor()
        
        print("ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_cached_function(self, ttl: int = None):
        """ìºì‹œëœ í•¨ìˆ˜ ë°ì½”ë ˆì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return cached(self.cache, ttl)
    
    def get_profiled_function(self, func_name: str = None):
        """í”„ë¡œíŒŒì¼ë§ëœ í•¨ìˆ˜ ë°ì½”ë ˆì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.monitor.profile_function(func_name)
    
    def evaluate_candidates_parallel(self, candidates: List[str],
                                   evaluation_func: Callable) -> List[Tuple[str, float]]:
        """í›„ë³´ë“¤ì„ ë³‘ë ¬ë¡œ í‰ê°€í•©ë‹ˆë‹¤."""
        return self.parallel_processor.evaluate_candidates_parallel(
            candidates, evaluation_func)
    
    def add_batch_update(self, update_type: str, **kwargs) -> None:
        """ë°°ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        if update_type == "word_relationship":
            self.batch_learning.add_word_relationship_update(**kwargs)
        elif update_type == "frequency":
            self.batch_learning.add_frequency_update(**kwargs)
        elif update_type == "pattern":
            self.batch_learning.add_pattern_update(kwargs)
    
    def get_comprehensive_performance_report(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # ê° ì»´í¬ë„ŒíŠ¸ í†µê³„ ìˆ˜ì§‘
        self.monitor.update_cache_performance(self.cache.get_stats())
        self.monitor.update_batch_performance(self.batch_learning.get_batch_statistics())
        self.monitor.record_memory_usage()
        
        return self.monitor.get_performance_report()
    
    def cleanup(self) -> None:
        """ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        self.batch_learning.force_process_all()
        self.parallel_processor.cleanup()
        self.monitor.stop_monitoring()
        print("ğŸ§¹ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")