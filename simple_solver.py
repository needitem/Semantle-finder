#!/usr/bin/env python3
"""
ì˜ë¯¸ ê¸°ë°˜ ì§€ëŠ¥í˜• ê¼¬ë§¨í‹€ ì†”ë²„
ì‹¤ì œ ìœ ì‚¬ë„ë¥¼ í•™ìŠµí•˜ì—¬ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í™œìš©í•œ ì •ë‹µ íƒìƒ‰
"""

import time
import random
import numpy as np
import pickle
import os
import json
from typing import List, Dict, Set, Tuple
from dataclasses import dataclass
from datetime import datetime

# --- User Configuration ---
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬: selenium, numpy
# pip install selenium numpy
# --------------------------

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
except ImportError as e:
    print(f"[ì˜¤ë¥˜] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ì†”ë²„ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— `pip install selenium numpy` ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit()

@dataclass
class GuessResult:
    word: str
    similarity: float
    rank: str = ""
    attempt: int = 0

class SemanticSolver:
    """ì˜ë¯¸ ê¸°ë°˜ ì§€ëŠ¥í˜• ê¼¬ë§¨í‹€ ì†”ë²„"""
    
    LEARNING_DATA_FILE = 'kkomantle_learning.json'
    WORD_PAIRS_FILE = 'word_pairs.json'

    def __init__(self):
        self.driver = None
        self.guesses: List[GuessResult] = []
        self.tried_words = set()
        
        self.vocab = self._load_korean_vocab_words()
        
        # ì§€ì†ì  í•™ìŠµ ë°ì´í„° ë¡œë“œ
        self.learning_data = self._load_learning_data()
        self.word_pairs = self._load_word_pairs()
        
        # í˜„ì¬ ì„¸ì…˜ ë°ì´í„°
        self.session_relationships = {}
        self.session_start = datetime.now()
        
        self.setup_driver()
        print(f"ğŸ§  ê¸°ì¡´ í•™ìŠµ ë°ì´í„°: {len(self.word_pairs)}ê°œ ë‹¨ì–´ ìŒ, {len(self.learning_data.get('successful_patterns', []))}ê°œ ì„±ê³µ íŒ¨í„´")

    def _load_korean_vocab_words(self) -> List[str]:
        """words.txtì—ì„œ ë‹¨ì–´ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open('words.txt', 'r', encoding='utf-8') as f:
                words = sorted(list(set(
                    line.strip()
                    for line in f
                    if line.strip() and not line.strip().startswith('[')
                )))
            print(f"âœ… words.txt ê¸°ë°˜ {len(words)}ê°œ ê³ ìœ  ë‹¨ì–´ ë¡œë“œ ì™„ë£Œ")
            if not words:
                raise FileNotFoundError
            return words
        except FileNotFoundError:
            print("âš ï¸ words.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹¨ì–´ ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return ["ì‚¬ë‘", "ì‹œê°„", "ì‚¬ëŒ", "ìƒê°", "ë§ˆìŒ", "ì„¸ìƒ", "ë¬¸ì œ", "ì‚¬íšŒ"]

    def _load_learning_data(self) -> Dict:
        """ì§€ì†ì  í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.LEARNING_DATA_FILE):
            try:
                with open(self.LEARNING_DATA_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return {
            'games_played': 0,
            'successful_patterns': [],
            'category_effectiveness': {},
            'word_frequency': {},
            'best_strategies': []
        }
    
    def _load_word_pairs(self) -> Dict[str, Dict[str, float]]:
        """ë‹¨ì–´ ìŒ ìœ ì‚¬ë„ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.WORD_PAIRS_FILE):
            try:
                with open(self.WORD_PAIRS_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ë‹¨ì–´ ìŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}
    
    def _save_learning_data(self):
        """í•™ìŠµ ë°ì´í„° ì €ì¥"""
        try:
            with open(self.LEARNING_DATA_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.learning_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ í•™ìŠµ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_word_pairs(self):
        """ë‹¨ì–´ ìŒ ë°ì´í„° ì €ì¥"""
        try:
            with open(self.WORD_PAIRS_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.word_pairs, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ë‹¨ì–´ ìŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    def setup_driver(self):
        """ë¸Œë¼ìš°ì € ì„¤ì •"""
        chrome_options = Options()
        # Headless ëª¨ë“œë¥¼ ë¹„í™œì„±í™”í•˜ë ¤ë©´ ì•„ë˜ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”. 
        # chrome_options.add_argument("--headless") 
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1200,800")
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            print("âœ… ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def select_next_word(self) -> str:
        """ì˜ë¯¸ ê¸°ë°˜ ì§€ëŠ¥í˜• ë‹¨ì–´ ì„ íƒ"""
        if not self.guesses:
            # ì´ˆê¸° ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë‹¨ì–´ë“¤ (ê· í˜•ì  íƒìƒ‰)
            initial_words = ["ì‚¬ëŒ", "ì‹œê°„", "ì‚¬ë‘", "ìì—°", "ìŒì‹", "ê¸°ìˆ ", "ê°ì •", "ì¥ì†Œ", "í–‰ë™", "ìƒê°"]
            available_initial = [w for w in initial_words if w in self.vocab and w not in self.tried_words]
            return random.choice(available_initial) if available_initial else self.vocab[0]

        # ìµœê³  ìœ ì‚¬ë„ì™€ ì§„í–‰ ìƒí™© ë¶„ì„
        best_similarity = max(g.similarity for g in self.guesses)
        
        # ì •ì²´ ìƒíƒœ ê°ì§€ (ìµœê·¼ 3ë²ˆ ì‹œë„ì—ì„œ ê°œì„ ì´ ì—†ìŒ)
        recent_attempts = self.guesses[-3:] if len(self.guesses) >= 3 else self.guesses
        recent_max = max(g.similarity for g in recent_attempts) if recent_attempts else 0
        is_stuck = len(self.guesses) >= 3 and recent_max <= best_similarity + 0.01
        
        # ì§€ëŠ¥í˜• íƒìƒ‰ ì „ëµ ê²°ì •
        if best_similarity < 0.1:
            return self._wide_semantic_exploration()
        elif best_similarity < 0.25 or is_stuck:
            if is_stuck:
                print("   âš ï¸ ì •ì²´ ìƒíƒœ - ì˜ë¯¸ ê³µê°„ í™•ì¥ íƒìƒ‰")
            return self._semantic_gradient_search()
        elif best_similarity < 0.5:
            return self._focused_semantic_search()
        else:
            return self._precision_semantic_search()

    def _wide_semantic_exploration(self) -> str:
        """ë„“ì€ ì˜ë¯¸ ê³µê°„ íƒìƒ‰"""
        print("   ğŸŒ ë„“ì€ ì˜ë¯¸ ê³µê°„ íƒìƒ‰")
        
        # ì˜ë¯¸ì ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ í•µì‹¬ ê°œë…ë“¤
        semantic_seeds = [
            # ì¶”ìƒ ê°œë…
            ["ìƒê°", "ë§ˆìŒ", "ì •ì‹ ", "ì˜ì‹", "ê°ì •", "ëŠë‚Œ"],
            # ë¬¼ë¦¬ì  ì¡´ì¬
            ["ì‚¬ë¬¼", "ë¬¼ê±´", "ë¬¼ì²´", "ì¡´ì¬", "ì‹¤ì²´", "í˜•íƒœ"],
            # ê´€ê³„ì™€ ì—°ê²°
            ["ê´€ê³„", "ì—°ê²°", "ê²°í•©", "ë§Œë‚¨", "ì†Œí†µ", "êµë¥˜"],
            # ë³€í™”ì™€ ê³¼ì •
            ["ë³€í™”", "ê³¼ì •", "ë°œì „", "ì„±ì¥", "ì§„í–‰", "íë¦„"],
            # ê³µê°„ê³¼ ìœ„ì¹˜
            ["ê³µê°„", "ì¥ì†Œ", "ìœ„ì¹˜", "ì§€ì—­", "ì˜ì—­", "ë²”ìœ„"],
            # ì‹œê°„ê³¼ ìˆœì„œ
            ["ì‹œê°„", "ìˆœê°„", "ì‹œê¸°", "ë•Œ", "ê¸°ê°„", "ìˆœì„œ"],
            # í–‰ë™ê³¼ í™œë™
            ["í–‰ë™", "í™œë™", "ì›€ì§ì„", "ì‘ì—…", "ë…¸ë ¥", "ì‹¤í–‰"],
            # ìƒíƒœì™€ ì¡°ê±´
            ["ìƒíƒœ", "ì¡°ê±´", "ìƒí™©", "í™˜ê²½", "ë¶„ìœ„ê¸°", "ê¸°ë¶„"]
        ]
        
        # ì´ë¯¸ ì‹œë„í•œ ë‹¨ì–´ë“¤ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì˜ì—­ ì„ íƒ
        tried_concepts = set()
        for guess in self.guesses:
            for i, concept_group in enumerate(semantic_seeds):
                if guess.word in concept_group:
                    tried_concepts.add(i)
        
        # ì‹œë„í•˜ì§€ ì•Šì€ ì˜ë¯¸ ì˜ì—­ì—ì„œ ì„ íƒ
        untried_concepts = [i for i in range(len(semantic_seeds)) if i not in tried_concepts]
        
        if untried_concepts:
            concept_idx = random.choice(untried_concepts)
            concept_words = [w for w in semantic_seeds[concept_idx] 
                           if w in self.vocab and w not in self.tried_words]
            if concept_words:
                next_word = random.choice(concept_words)
                print(f"   ğŸ¯ ìƒˆë¡œìš´ ì˜ë¯¸ ì˜ì—­ íƒìƒ‰: '{next_word}'")
                return next_word
        
        # ëª¨ë“  í•µì‹¬ ê°œë…ì„ ì‹œë„í–ˆë‹¤ë©´, íŒŒìƒì–´ íƒìƒ‰
        return self._explore_word_derivatives()
    
    def _explore_word_derivatives(self) -> str:
        """ê¸°ì¡´ ë‹¨ì–´ë“¤ì˜ íŒŒìƒì–´ íƒìƒ‰"""
        derivatives = []
        
        for guess in self.guesses:
            word = guess.word
            # ì–´ê·¼ ê¸°ë°˜ íŒŒìƒì–´
            if len(word) > 1:
                root = word[:-1]
                for vocab_word in self.vocab:
                    if (vocab_word.startswith(root) and vocab_word != word and 
                        vocab_word not in self.tried_words and len(vocab_word) <= len(word) + 2):
                        derivatives.append((vocab_word, abs(guess.similarity - 0.1)))  # ì˜ˆìƒ ìœ ì‚¬ë„ ì°¨ì´
        
        if derivatives:
            # ìœ ì‚¬ë„ê°€ ë†’ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ìˆœìœ¼ë¡œ ì •ë ¬
            derivatives.sort(key=lambda x: x[1])
            next_word = derivatives[0][0]
            print(f"   ğŸ¯ íŒŒìƒì–´ íƒìƒ‰: '{next_word}'")
            return next_word
        
        # íŒŒìƒì–´ê°€ ì—†ìœ¼ë©´ ëœë¤
        available_words = [w for w in self.vocab if w not in self.tried_words]
        return random.choice(available_words) if available_words else "í¬ê¸°"

    def _semantic_gradient_search(self) -> str:
        """ì˜ë¯¸ì  ê²½ì‚¬ ìƒìŠ¹ë²• íƒìƒ‰"""
        print("   ğŸ“ˆ ì˜ë¯¸ì  ê²½ì‚¬ íƒìƒ‰")
        
        # ìƒìœ„ ìœ ì‚¬ë„ ë‹¨ì–´ë“¤ë¡œ ë°©í–¥ì„± ì°¾ê¸°
        top_guesses = sorted(self.guesses, key=lambda g: g.similarity, reverse=True)[:3]
        
        # ì—¬ëŸ¬ ë°©í–¥ ë™ì‹œ íƒìƒ‰
        search_directions = []
        
        for guess in top_guesses:
            # 1. ì˜ë¯¸ì  í™•ì¥ (ìƒìœ„ì–´, í•˜ìœ„ì–´)
            search_directions.extend(self._get_semantic_expansions(guess.word))
            
            # 2. ì—°ê´€ì–´ íƒìƒ‰ (ë™ì˜ì–´, ìœ ì˜ì–´)
            search_directions.extend(self._get_semantic_associations(guess.word))
            
            # 3. ë¬¸ë§¥ì  ê´€ë ¨ì–´
            search_directions.extend(self._get_contextual_relations(guess.word))
        
        # ì¤‘ë³µ ì œê±° ë° ë¯¸ì‹œë„ ë‹¨ì–´ë§Œ
        candidates = list(set(search_directions) - self.tried_words)
        candidates = [w for w in candidates if w in self.vocab]
        
        if candidates:
            # í•™ìŠµëœ íš¨ê³¼ì„±ìœ¼ë¡œ ì •ë ¬
            candidates = self._sort_by_learned_effectiveness("", candidates)
            next_word = candidates[0] if candidates else random.choice(candidates)
            print(f"   ğŸ¯ ì˜ë¯¸ì  ë°©í–¥ íƒìƒ‰: '{next_word}'")
            return next_word
        
        # í›„ë³´ê°€ ì—†ìœ¼ë©´ ë„“ì€ íƒìƒ‰ìœ¼ë¡œ ì „í™˜
        return self._wide_semantic_exploration()
    
    def _get_semantic_expansions(self, word: str) -> List[str]:
        """ì˜ë¯¸ì  í™•ì¥ì–´ ìƒì„±"""
        expansions = []
        
        # ì–´ê·¼ ê¸°ë°˜ í™•ì¥
        if len(word) > 2:
            root = word[:2]  # ì• 2ê¸€ì ì–´ê·¼
            for vocab_word in self.vocab:
                if vocab_word.startswith(root) and vocab_word != word:
                    expansions.append(vocab_word)
        
        # ì˜ë¯¸ ì˜ì—­ë³„ í™•ì¥
        semantic_expansions = {
            "ì‚¬ëŒ": ["ì¸ê°„", "ê°œì¸", "íƒ€ì¸", "ëˆ„êµ°ê°€", "ì‚¬ëŒë“¤", "ì¸ë¬¼", "ì¸ì‚¬"],
            "ì‹œê°„": ["ë•Œ", "ìˆœê°„", "ì‹œê¸°", "ì‹œì ˆ", "ê¸°ê°„", "ì‹œì ", "ì‹œëŒ€"],
            "ì¥ì†Œ": ["ê³³", "ì§€ì—­", "ìœ„ì¹˜", "ê³µê°„", "ì˜ì—­", "ë²”ìœ„", "ì˜í† "],
            "ë°©ë²•": ["ìˆ˜ë‹¨", "ë°©ì‹", "ê¸°ë²•", "ì ˆì°¨", "ê³¼ì •", "ë‹¨ê³„"],
            "ìƒíƒœ": ["ì¡°ê±´", "ìƒí™©", "í™˜ê²½", "ë¶„ìœ„ê¸°", "ëŠë‚Œ", "ê¸°ë¶„"],
            "í–‰ë™": ["í™œë™", "ì›€ì§ì„", "ì‘ì—…", "í–‰ìœ„", "ì‹¤í–‰", "ì§„í–‰"]
        }
        
        for category, words in semantic_expansions.items():
            if word in words:
                expansions.extend([w for w in words if w != word])
        
        return expansions[:10]
    
    def _get_semantic_associations(self, word: str) -> List[str]:
        """ì˜ë¯¸ì  ì—°ê´€ì–´ ìƒì„±"""
        associations = []
        
        # í•™ìŠµëœ ì—°ê´€ì–´ ìš°ì„ 
        learned_assocs = self._get_learned_related_words(word)
        associations.extend(learned_assocs)
        
        # ì–¸ì–´í•™ì  ì—°ê´€ì–´
        linguistic_associations = {
            # ë™ì˜ì–´/ìœ ì˜ì–´ ê·¸ë£¹
            "ë¬¸ì œ": ["ê³¼ì œ", "ìŸì ", "ì´ìŠˆ", "ê³ ë¯¼", "ê±±ì •", "ë‚œì œ"],
            "í•´ê²°": ["ì²˜ë¦¬", "í•´ë‹µ", "ë°©ì•ˆ", "ëŒ€ì•ˆ", "ê·¹ë³µ", "ì™„ë£Œ"],
            "ì¤‘ìš”": ["í•µì‹¬", "ì£¼ìš”", "í•„ìˆ˜", "ê¸°ë³¸", "ê·¼ë³¸", "ë³¸ì§ˆ"],
            "ë³€í™”": ["ì „í™˜", "ê°œì„ ", "ë°œì „", "ì§„ë³´", "ì„±ì¥", "í˜ì‹ "],
            "ê´€ê³„": ["ì—°ê²°", "ê²°í•©", "ì†Œí†µ", "êµë¥˜", "ìƒí˜¸ì‘ìš©", "í˜‘ë ¥"]
        }
        
        for key, words in linguistic_associations.items():
            if word == key or word in words:
                associations.extend([w for w in words if w != word])
        
        return associations[:8]
    
    def _get_contextual_relations(self, word: str) -> List[str]:
        """ë¬¸ë§¥ì  ê´€ë ¨ì–´ ìƒì„±"""
        relations = []
        
        # ë¬¸ë§¥ë³„ ê´€ë ¨ì–´ ë§¤í•‘
        contextual_maps = {
            # ì‚¬íšŒ/ì •ì¹˜ ë¬¸ë§¥
            "ì‚¬íšŒ": ["ì •ì¹˜", "ê²½ì œ", "ë¬¸í™”", "êµìœ¡", "ë³µì§€", "ì œë„"],
            "êµ­ë¯¼": ["ì‹œë¯¼", "ì£¼ë¯¼", "ì¸ë¯¼", "êµ­ê°€", "ì •ë¶€", "ê³µë™ì²´"],
            
            # êµìœ¡/í•™ìŠµ ë¬¸ë§¥  
            "í•™ìŠµ": ["êµìœ¡", "ê³µë¶€", "ì—°êµ¬", "ì§€ì‹", "ì´í•´", "ìŠµë“"],
            "ì§€ì‹": ["ì •ë³´", "í•™ë¬¸", "ê²½í—˜", "ê¸°ìˆ ", "ëŠ¥ë ¥", "ì‹¤ë ¥"],
            
            # ê°ì •/ì‹¬ë¦¬ ë¬¸ë§¥
            "ê°ì •": ["ë§ˆìŒ", "ê¸°ë¶„", "ëŠë‚Œ", "ì •ì„œ", "ì‹¬ë¦¬", "ì˜ì‹"],
            "í–‰ë³µ": ["ê¸°ì¨", "ë§Œì¡±", "ì¦ê±°ì›€", "ì›ƒìŒ", "í‰í™”", "ì‚¬ë‘"]
        }
        
        for key, words in contextual_maps.items():
            if word == key or word in words:
                relations.extend([w for w in words if w != word])
        
        return relations[:6]

    def _focused_semantic_search(self) -> str:
        """ì§‘ì¤‘ì  ì˜ë¯¸ íƒìƒ‰"""
        print("   ğŸ¯ ì§‘ì¤‘ ì˜ë¯¸ íƒìƒ‰")
        
        # ìƒìœ„ ìœ ì‚¬ë„ ë‹¨ì–´ë“¤ ë¶„ì„
        top_guesses = sorted(self.guesses, key=lambda g: g.similarity, reverse=True)[:2]
        
        # ê³ ìœ ì‚¬ë„ ë‹¨ì–´ë“¤ì˜ ê³µí†µ ì˜ë¯¸ ì˜ì—­ ì°¾ê¸°
        common_semantic_field = self._find_common_semantic_field([g.word for g in top_guesses])
        
        if common_semantic_field:
            candidates = [w for w in common_semantic_field 
                         if w in self.vocab and w not in self.tried_words]
            if candidates:
                next_word = candidates[0]
                print(f"   ğŸ¯ ê³µí†µ ì˜ë¯¸ ì˜ì—­: '{next_word}'")
                return next_word
        
        # ìµœê³  ë‹¨ì–´ ì£¼ë³€ ì§‘ì¤‘ íƒìƒ‰
        best_guess = max(self.guesses, key=lambda g: g.similarity)
        
        # ë‹¤ì¸µì  ì—°ê´€ì–´ íƒìƒ‰
        layer1 = self._get_semantic_associations(best_guess.word)
        layer2 = []
        for word in layer1[:3]:
            if word not in self.tried_words:
                layer2.extend(self._get_semantic_associations(word))
        
        all_candidates = layer1 + layer2
        candidates = [w for w in set(all_candidates) 
                     if w in self.vocab and w not in self.tried_words]
        
        if candidates:
            # í•™ìŠµëœ íš¨ê³¼ì„±ìœ¼ë¡œ ì •ë ¬
            candidates = self._sort_by_learned_effectiveness(best_guess.word, candidates)
            next_word = candidates[0]
            print(f"   ğŸ¯ '{best_guess.word}' ë‹¤ì¸µ ì—°ê´€ì–´: '{next_word}'")
            return next_word
        
        return self._semantic_gradient_search()
    
    def _precision_semantic_search(self) -> str:
        """ì •ë°€ ì˜ë¯¸ íƒìƒ‰ (ê³ ìœ ì‚¬ë„ ìƒí™©)"""
        print("   ğŸ”¬ ì •ë°€ ì˜ë¯¸ íƒìƒ‰")
        
        best_guess = max(self.guesses, key=lambda g: g.similarity)
        
        # ë¯¸ì„¸í•œ ì˜ë¯¸ ì°¨ì´ íƒìƒ‰
        precision_candidates = []
        
        # 1. ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ë³€í˜•
        word = best_guess.word
        morphological_variants = []
        
        if len(word) > 2:
            # ì ‘ë¯¸ì‚¬ ë³€í˜•
            root = word[:-1]
            for suffix in ['ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì ', 'ì˜', 'ë¡œ', 'ì„', 'ë¥¼']:
                variant = root + suffix
                if variant in self.vocab and variant not in self.tried_words:
                    morphological_variants.append(variant)
        
        precision_candidates.extend(morphological_variants[:3])
        
        # 2. í•™ìŠµëœ ì´ˆê·¼ì ‘ ë‹¨ì–´ë“¤
        ultra_close_words = []
        for pair_key, pair_data in self.word_pairs.items():
            word1, word2 = pair_key.split('|')
            
            if (word1 == word or word2 == word):
                other_word = word2 if word1 == word else word1
                if other_word in self.vocab and other_word not in self.tried_words:
                    avg_diff = sum(pair_data['similarity_diffs']) / len(pair_data['similarity_diffs'])
                    if avg_diff < 0.03:  # ë§¤ìš° ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ë§Œ
                        ultra_close_words.append((other_word, avg_diff))
        
        # ìœ ì‚¬ë„ ì°¨ì´ ìˆœìœ¼ë¡œ ì •ë ¬
        ultra_close_words.sort(key=lambda x: x[1])
        precision_candidates.extend([w[0] for w in ultra_close_words[:2]])
        
        if precision_candidates:
            next_word = precision_candidates[0]
            print(f"   ğŸ¯ '{word}' ì •ë°€ ë³€í˜•: '{next_word}'")
            return next_word
        
        # ì •ë°€ íƒìƒ‰ ì‹¤íŒ¨ ì‹œ ì§‘ì¤‘ íƒìƒ‰ìœ¼ë¡œ ë³µê·€
        return self._focused_semantic_search()
    
    def _find_common_semantic_field(self, words: List[str]) -> List[str]:
        """ë‹¨ì–´ë“¤ì˜ ê³µí†µ ì˜ë¯¸ ì˜ì—­ ì°¾ê¸°"""
        if len(words) < 2:
            return []
        
        # ì˜ë¯¸ ì˜ì—­ë³„ ë‹¨ì–´ ê·¸ë£¹
        semantic_fields = {
            "ì •ì¹˜ì‚¬íšŒ": ["ì •ì¹˜", "ì‚¬íšŒ", "êµ­ê°€", "ì •ë¶€", "êµ­ë¯¼", "ì‹œë¯¼", "ê³µë™ì²´", "ì‚¬íšŒì ", "ì •ì±…", "ì œë„"],
            "êµìœ¡í•™ìŠµ": ["êµìœ¡", "í•™ìŠµ", "ê³µë¶€", "ì§€ì‹", "í•™ë¬¸", "ì—°êµ¬", "ì´í•´", "ìŠµë“", "ê²½í—˜"],
            "ê°ì •ì‹¬ë¦¬": ["ê°ì •", "ë§ˆìŒ", "ê¸°ë¶„", "ëŠë‚Œ", "ì •ì„œ", "ì‹¬ë¦¬", "ì‚¬ë‘", "í–‰ë³µ", "ìŠ¬í””"],
            "ì‹œê³µê°„": ["ì‹œê°„", "ê³µê°„", "ì¥ì†Œ", "ìœ„ì¹˜", "ë•Œ", "ìˆœê°„", "ì§€ì—­", "ì˜ì—­", "ë²”ìœ„"],
            "í–‰ë™í™œë™": ["í–‰ë™", "í™œë™", "ì›€ì§ì„", "ì‘ì—…", "ì‹¤í–‰", "ì§„í–‰", "ê³¼ì •", "ë°©ë²•"]
        }
        
        for field_name, field_words in semantic_fields.items():
            if all(any(word in field_words or word.startswith(fw[:2]) for fw in field_words) for word in words):
                return [w for w in field_words if w not in words]
        
        return []

    def _get_learned_related_words(self, word: str) -> List[str]:
        """í•™ìŠµëœ ë°ì´í„°ì—ì„œ ê´€ë ¨ì–´ ì°¾ê¸°"""
        candidates = []
        
        for pair_key, pair_data in self.word_pairs.items():
            word1, word2 = pair_key.split('|')
            
            if word1 == word and word2 in self.vocab and word2 not in self.tried_words:
                # ìœ ì‚¬ë„ ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ
                avg_diff = sum(pair_data['similarity_diffs']) / len(pair_data['similarity_diffs'])
                if avg_diff < 0.1:  # ìœ ì‚¬ë„ ì°¨ì´ê°€ 0.1 ë¯¸ë§Œì¸ ê²½ìš°ë§Œ
                    candidates.append(word2)
            elif word2 == word and word1 in self.vocab and word1 not in self.tried_words:
                avg_diff = sum(pair_data['similarity_diffs']) / len(pair_data['similarity_diffs'])
                if avg_diff < 0.1:
                    candidates.append(word1)
        
        return candidates
    
    def _sort_by_learned_effectiveness(self, base_word: str, candidates: List[str]) -> List[str]:
        """í•™ìŠµëœ íš¨ê³¼ì„±ì— ë”°ë¼ í›„ë³´ ë‹¨ì–´ë“¤ ì •ë ¬"""
        def get_effectiveness_score(candidate: str) -> float:
            # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ ì ìˆ˜
            if candidate in self.learning_data.get('word_frequency', {}):
                freq_data = self.learning_data['word_frequency'][candidate]
                return freq_data.get('avg_similarity', 0) * freq_data.get('count', 1)
            return 0
        
        candidates.sort(key=get_effectiveness_score, reverse=True)
        return candidates

    def _remove_word_from_vocab(self, word: str):
        """ì‹¤íŒ¨í•œ ë‹¨ì–´ë¥¼ ì–´íœ˜ì—ì„œ ì œê±°"""
        if word in self.vocab:
            self.vocab.remove(word)
            self.tried_words.add(word)
            print(f"  âš ï¸ ë‹¨ì–´ '{word}' ì–´íœ˜ì—ì„œ ì˜êµ¬ ì œê±°")

    def navigate_to_game(self) -> bool:
        try: self.driver.get("https://semantle-ko.newsjel.ly/"); time.sleep(1); return True
        except Exception: return False

    def submit_word(self, word: str) -> bool:
        try:
            self.driver.execute_script(f"document.getElementById('guess_input').value = '{word}'; document.querySelector('.input-wrapper button').click();")
            time.sleep(0.005)  # ì´ˆê·¹ì†Œ ëŒ€ê¸°
            return True
        except Exception: return False

    def parse_result(self, word: str, attempt: int) -> GuessResult:
        # ì´ˆê·¹ì†Œ ëŒ€ê¸° ì‹œê°„
        time.sleep(0.005)
        
        # ë‹¨ì¼ ì‹œë„ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
        try:
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            table_exists = self.driver.execute_script("return document.getElementById('guesses_table') !== null;")
            if not table_exists:
                print(f"  - í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë‹¨ì–´ '{word}' ì œê±°")
                self._remove_word_from_vocab(word)
                return None

            # last-input í´ë˜ìŠ¤ í–‰ì—ì„œ ìµœì‹  ì…ë ¥ ì°¾ê¸°
            script = f"""
            var table = document.getElementById('guesses_table');
            if (!table) return null;

            // last-input í´ë˜ìŠ¤ê°€ ìˆëŠ” í–‰ ì°¾ê¸° (ìµœì‹  ì…ë ¥)
            var lastInputRow = table.querySelector('tbody tr.last-input');
            if (lastInputRow) {{
                var cells = lastInputRow.querySelectorAll('td');
                if (cells.length >= 4) {{
                    var cellWord = cells[1] ? cells[1].textContent.trim() : '';
                    console.log('last-input í–‰ ë‹¨ì–´:', cellWord, 'ì°¾ëŠ” ë‹¨ì–´:', '{word}');
                    
                    if (cellWord === '{word}') {{
                        var similarity = cells[2] ? cells[2].textContent.trim() : '';
                        var rank = cells[3] ? cells[3].textContent.trim() : '1000ìœ„ ì´ìƒ';
                        console.log('ì°¾ìŒ! ìœ ì‚¬ë„:', similarity, 'ìˆœìœ„:', rank);
                        
                        return {{
                            word: cellWord,
                            similarity: similarity,
                            rank: rank
                        }};
                    }}
                }}
            }}
            
            // last-inputì´ ì—†ìœ¼ë©´ ì „ì²´ í…Œì´ë¸”ì—ì„œ ê²€ìƒ‰
            var rows = table.querySelectorAll('tbody tr:not(.delimiter)');
            for (var i = rows.length - 1; i >= 0; i--) {{
                var cells = rows[i].querySelectorAll('td');
                if (cells.length >= 4) {{
                    var cellWord = cells[1] ? cells[1].textContent.trim() : '';
                    if (cellWord === '{word}') {{
                        var similarity = cells[2] ? cells[2].textContent.trim() : '';
                        var rank = cells[3] ? cells[3].textContent.trim() : '1000ìœ„ ì´ìƒ';
                        return {{
                            word: cellWord,
                            similarity: similarity,
                            rank: rank
                        }};
                    }}
                }}
            }}
            return null;
            """

            result = self.driver.execute_script(script)
            if result:
                sim_text = result['similarity'].strip()
                
                # ë¹ ë¥¸ ì—¬ëŸ¬ í˜•íƒœì˜ ìœ ì‚¬ë„ ê°’ ì²˜ë¦¬
                if '%' in sim_text:
                    similarity = float(sim_text.replace('%', '')) / 100.0
                elif sim_text.replace('.', '').replace('-', '').isdigit():
                    sim_value = float(sim_text)
                    similarity = sim_value / 100.0 if sim_value > 1.0 else sim_value
                else:
                    similarity = 0.0
                
                guess = GuessResult(word, similarity, result['rank'], attempt)
                self.guesses.append(guess)
                self.guesses.sort(key=lambda g: g.similarity, reverse=True)
                self.tried_words.add(word)
                
                # ë‹¨ì–´ ê´€ê³„ í•™ìŠµ ì—…ë°ì´íŠ¸
                self._learn_word_relationships(word, similarity)
                
                return guess
            else:
                print(f"  - ë‹¨ì–´ '{word}' ê²°ê³¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë‹¨ì–´ ì œê±°")
                self._remove_word_from_vocab(word)
                return None
        except Exception as e:
            print(f"  - íŒŒì‹± ì˜¤ë¥˜: {e} - ë‹¨ì–´ '{word}' ì œê±°")
            self._remove_word_from_vocab(word)
            return None

    def _learn_word_relationships(self, word: str, similarity: float):
        """ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê´€ê³„ë¥¼ í•™ìŠµ (ì§€ì†ì  + ì„¸ì…˜)"""
        # í˜„ì¬ ì„¸ì…˜ ë°ì´í„° ì—…ë°ì´íŠ¸
        if word not in self.session_relationships:
            self.session_relationships[word] = []
        
        # ì´ë¯¸ ì‹œë„í•œ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ ì—…ë°ì´íŠ¸
        for guess in self.guesses:
            if guess.word != word:
                similarity_diff = abs(similarity - guess.similarity)
                self.session_relationships[word].append((guess.word, similarity_diff))
                
                # ì§€ì†ì  ë°ì´í„°ì—ë„ ì €ì¥ (ì–‘ë°©í–¥)
                pair_key = f"{min(word, guess.word)}|{max(word, guess.word)}"
                if pair_key not in self.word_pairs:
                    self.word_pairs[pair_key] = {
                        'similarity_diffs': [],
                        'co_occurrence_count': 0
                    }
                
                self.word_pairs[pair_key]['similarity_diffs'].append(similarity_diff)
                self.word_pairs[pair_key]['co_occurrence_count'] += 1
                
                # ìµœëŒ€ 100ê°œ ê¸°ë¡ë§Œ ìœ ì§€
                if len(self.word_pairs[pair_key]['similarity_diffs']) > 100:
                    self.word_pairs[pair_key]['similarity_diffs'] = self.word_pairs[pair_key]['similarity_diffs'][-50:]
        
        # ë‹¨ì–´ ë¹ˆë„ ì—…ë°ì´íŠ¸
        if 'word_frequency' not in self.learning_data:
            self.learning_data['word_frequency'] = {}
        
        if word not in self.learning_data['word_frequency']:
            self.learning_data['word_frequency'][word] = {'count': 0, 'avg_similarity': 0, 'best_similarity': 0}
        
        freq_data = self.learning_data['word_frequency'][word]
        freq_data['count'] += 1
        freq_data['avg_similarity'] = (freq_data['avg_similarity'] * (freq_data['count'] - 1) + similarity) / freq_data['count']
        freq_data['best_similarity'] = max(freq_data['best_similarity'], similarity)

    def solve(self):
        if not self.navigate_to_game(): return
        print("\n" + "="*50)
        print("ğŸ¯ ì‹¤ì œ ìœ ì‚¬ë„ í•™ìŠµ ê¸°ë°˜ ì˜ë¯¸ì  ì†”ë²„ ì‹œì‘")
        print(f"ğŸ“š ë‹¨ì–´ í’€: {len(self.vocab)}ê°œ")
        print("="*50 + "\n")
        
        start_time = time.time()
        failed_words = set()  # íŒŒì‹± ì‹¤íŒ¨í•œ ë‹¨ì–´ë“¤ ì¶”ì 
        
        for attempt in range(1, 501):
            word = self.select_next_word()
            if word == "í¬ê¸°": print("âš ï¸ ëª¨ë“  ë‹¨ì–´ë¥¼ ì‹œë„í•˜ì—¬ ë” ì´ìƒ ì¶”ì¸¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); break

            # ì´ë¯¸ ì‹¤íŒ¨í•œ ë‹¨ì–´ëŠ” ìŠ¤í‚µ
            if word in failed_words:
                print(f"âš ï¸ ë‹¨ì–´ '{word}' ì´ë¯¸ íŒŒì‹± ì‹¤íŒ¨ë¡œ ìŠ¤í‚µë¨. ë‹¤ë¥¸ ë‹¨ì–´ ì„ íƒ...")
                self.tried_words.add(word)  # ì‹œë„ ëª©ë¡ì— ì¶”ê°€í•˜ì—¬ ë‹¤ì‹œ ì„ íƒë˜ì§€ ì•Šë„ë¡
                continue

            print(f"ğŸ¯ ì‹œë„ {attempt}: '{word}'")
            if not self.submit_word(word):
                print("âŒ ì œì¶œ ì‹¤íŒ¨. 3ì´ˆ í›„ ì¬ì‹œë„..."); time.sleep(3); continue

            result = self.parse_result(word, attempt)
            if not result:
                print(f"âŒ ë‹¨ì–´ '{word}' ì²˜ë¦¬ ì‹¤íŒ¨ - ì–´íœ˜ì—ì„œ ì œê±°ë¨")
                continue

            print(f"   ğŸ“Š ê²°ê³¼: ìœ ì‚¬ë„ {result.similarity:.4f} | ìˆœìœ„: {result.rank}")
            if result.similarity == 1.0:
                print(f"\nğŸ‰ ì •ë‹µ! '{word}' (ì‹œë„: {attempt}, ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ)")
                self._save_session_results(success=True, final_answer=word)
                return word
        print("ì†”ë²„ê°€ 500íšŒ ì‹œë„ í›„ ì •ë‹µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    def _save_session_results(self, success: bool = False, final_answer: str = None):
        """ì„¸ì…˜ ê²°ê³¼ ì €ì¥ ë° í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        # ê²Œì„ íšŸìˆ˜ ì—…ë°ì´íŠ¸
        self.learning_data['games_played'] += 1
        
        # ì„±ê³µ íŒ¨í„´ ì €ì¥
        if success and final_answer:
            session_pattern = {
                'answer': final_answer,
                'attempts': len(self.guesses),
                'strategy_sequence': [],  # ì‚¬ìš©í•œ ì „ëµ ìˆœì„œ
                'key_words': [g.word for g in sorted(self.guesses, key=lambda x: x.similarity, reverse=True)[:5]],
                'timestamp': datetime.now().isoformat()
            }
            
            if 'successful_patterns' not in self.learning_data:
                self.learning_data['successful_patterns'] = []
            
            self.learning_data['successful_patterns'].append(session_pattern)
            
            # ìµœëŒ€ 100ê°œ ì„±ê³µ íŒ¨í„´ë§Œ ìœ ì§€
            if len(self.learning_data['successful_patterns']) > 100:
                self.learning_data['successful_patterns'] = self.learning_data['successful_patterns'][-50:]
        
        # ë°ì´í„° ì €ì¥
        self._save_learning_data()
        self._save_word_pairs()
        
        print(f"ğŸ’¾ í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ (total games: {self.learning_data['games_played']})")
    
    def cleanup(self):
        # ë¸Œë¼ìš°ì €ëŠ” ì¢…ë£Œí•˜ì§€ ì•ŠìŒ (ì‚¬ìš©ì ìš”ì²­)
        # if self.driver: 
        #     self.driver.quit()
        # ì„¸ì…˜ ì¢…ë£Œ ì‹œ ë°ì´í„° ì €ì¥
        self._save_session_results()

def main():
    print("ğŸš€ ì˜ë¯¸ ê¸°ë°˜ ì§€ëŠ¥í˜• ê¼¬ë§¨í‹€ ì†”ë²„")
    solver = None
    try:
        solver = SemanticSolver()
        solver.solve()
    except (KeyboardInterrupt, SystemExit):
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if solver: solver.cleanup()
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()